Background
Dynamic Neural Networks
Neural Engineering Framework
RNN
Reservoir Computing
FORCE, full-force, duggins, nicola
LSTM, maass, cortical microcircuits, GRUs
Hugh & Sjenowski
Unsupervised (SORN / Numenta)
Summary (Table)
Control-Theoretic Architectures
Slotine
Predictive Coding / Deneve / Memmesheimer
Aditya & Gerstner
Friedemann Zen
Recurrent Information Optimization with Local, Metaplastic Synaptic Dynamics (ShiNung Ching)
SLA https://arxiv.org/abs/1701.05880
Neuromorphic Computing
Spiked-based computation goals (compare against Craig Vineyard sorting)
Colocated memory and computation
Minimize spiking / synaptic events
Class of computations / descriptions
SpiNNaker 1 and 2
Neurogrid
Braindrop / Brainstorm
Loihi
Spikey, BrainScaleS 1 and 2
Dynapse 1 and 2
TrueNorth
DeepSouth
Other Circuits COLAMN, ODIN, ROLLS
Giacomo Indiverdi & Eliasmith
Tripp
Wang and Tapson
STPU https://arxiv.org/pdf/1704.08306.pdf
Neuromemristive random projection network Dhireesha Kudithipudi
Summary (Table)

Meta
Venn diagram (HW/Biology/Useful)
Rate coding? - sLIF <-> rLIF equivalence - quantifying difference as function of (n, tau)
Temporal solver with LIF and others
Spike coding / rank-order coding (simon thorpe) / PSC coding / http://compneuro.uwaterloo.ca/files/publications/tripp.2007.pdf
Input frequency of relevant information is limited by dendritic computation; assuming low frequency for now since lowpass dominates
Reduction to single high dimensional recurrent layer
Optimality of low-rank representation (any linear transform is either 1 or 2 matrices. More can be collapsed)
Dimensions in the brain (Konrad Kording). 100 per dimension (Deneve). “Low”-dimensional is a relative term
Duality between microscopic and macroscopic dynamics (for example, implementing the LIF dynamics using a pool of LIF) -- principle of compositionality: using an entire network as a “virtual synapse” to another pool 
Continuous versus discrete
https://ieeexplore.ieee.org/document/7551407/ Christian Mayr’s digital SRAM scaling ISCA
Delays in action and vision (John Doyle)
Heterogeneity in time (physics, biological detail, LSTMs and multiple time-scales -- versus the traditional heterogeneity in space i.e., different static response curves)
Power considerations in biology, neuromorphics, and attentional control

Dynamical Systems (both discrete and continuous)
Linear Time-Invariant Systems
State-Space Representation
Transfer Function
Properties
Nonlinear Systems
Linearization
Jacobians
Chaos
Pade Approximants
Coordinate Transform for 1/p(s) form 

Analysis
Principle 3
Learning as a Dynamical System
Saturation as a Dynamical System
Noise / decoding error as a Dynamical System
Visualizing error in recurrent function representation
Dynamics with injected noise (linear feedback) for rate and spiking
Spike-crossing / tau_rc
Uniform ISI assumption
Noise as a function of # neurons / firing rate / tau
Spike-time Chaos (Theoretical justification for NEF versus Deneve)

Methodology
Synapse Models
Recurrent Architecture (Principle 3)
Nengo
Zero-order hold spiking LIF
Multi-spiking LIF
nengolib
Online versus offline learning
Spike versus rate learning

Geometric Perspectives
Encoders, Evaluation Points, and Semantic Pointers
Sampling Vectors and Coordinates
Quasi-Monte Carlo Sampling (Eval Points [and encoders])
Voja Learning (Encoders)
Leech Lattice (Semantic Pointers)
Vector rounding to +/- 1
Locality sensitive hashing
Geometric Decoder Optimization
State-Space Realizations

Delay Network
Motivation
Naive Attempts
Optimal Solution
Basis Functions (Temporal Code)
RC Comparison
Time Cell Comparison
Applications
Detecting cyclic structure
Autocorrelation (Cosyne talk used this)
Replay
A General Unsolved Theory (Recurrent compression/representation of arbitrary signals described by some model)

Applications to Optimization Problems
Compressed sensing
Locally competitive algorithm (LASSO / Mike Davies)
Gradient descent
Expectation-maximization

Extensions to Arbitrary Linear Synapses
Linear Transfer Function Characterization
Nonlinear State-space Characterization
Tau hetereogeneity (per post-neuron [braindrop] or per pre-neuron [differentiation])
Solving for optimal filter + decoders simultaneously

Extensions to Detailed Neuron Models
Perfect solution to adaptive LIF
Divisive adaptive LIF (https://github.com/nengo/nengo/issues/1423)
Less trivial cases?
Bioneurons?
Wilson neuron? RK45
Conductance-based synapses
Dale's principle

Applications
SpiNNaker Voja
Braindrop Mixed-Analog Digital Synapses
Tactile Classification
Winner-Take-All Networks

