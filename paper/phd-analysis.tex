\chapter{Analysis of the Neural Engineering Framework}
\label{chapt:analysis}

This section is intended to talk about high-level NEF considerations / analysis that don't fit elsewhere in the thesis, aren't published elsewhere, help compare it with other frameworks, and illuminate what it is doing from a computational perspective.

\section{Computational Principles}

An ode to the three principles of the NEF.
These are themes that primarily expand upon the third principle.

\subsection{Single-Layer Equivalence}

The idea that any multi-layer network can be collapsed into a single recurrently connected layer.
In the NEF, this implies a certain structure with sparse encoders and sparse decoders.

\subsection{Heterogeneous Dynamical Primitives}

The idea that synapses, neurons, and even entire recurrent networks, can all be described in terms of their filtering / dynamical equations.
Of core importance is the notion that we want a rich basis of dynamics that is heterogeneous in time.

\subsection{Duality of Macroscopic and Microscopic Dynamics}

The corollary that, once all things are expressed in the same language, they become somewhat interchangeable.
Examples: (1) we can implement a lowpass filter as a synapse or as a network, (2) we can implement spiking LIF voltage dynamics as a single neuron or as a network.
But this also goes the other way (hence the duality).
Consider the architecture: $A \rightarrow (B \rightarrow B) \rightarrow A$, where $A$ and $B$ are neural ensembles.
We can think of $B \rightarrow B$ as a dynamical primitive, and then re-interpret the whole system as $A \rightarrow A$ with a much more complicated synapse model described by $B \rightarrow B$.
Like a ``virtual'' synapse, implemented by an entire sub-network.
The practical implication is that we can use the same theory and the same tools to understand and compose all of these things.
For example, if $B \rightarrow B$ is a delay network, we can then use what we know about amplifying axonal spike delays to make an even longer delay out of the entire system (see section~\ref{sec:delay-applications})

\subsection{Continuous-Time and Discrete-Time}

The idea that both continuous-time and discrete-time dynamics play important roles at different time-scales, both in terms of the dominant dynamics and in terms of the desired computation.
It is also important to consider both for neuromorphic hardware.

\subsection{Post-Synaptic Current Coding}

The NEF's state variable is linearly mapped onto the post-synaptic current.
Contrast this with rate code and spike-time code and rank-order coding (simon thorpe) and Bryan Tripp (2007).
We further step outside this paradigm for conductance-based synapses, and adaptive LIF neurons.

\subsection{Low-Frequency Representations}

The idea that the most amount of ``information'' is present within the low-frequency range of the signals, relative to the firing rates of neurons and time-constants of synapses.

Maybe additional biological detail can get around this limitation, or maybe the time-constants are this way for this very reason.


\section{Suitability for Neuromorphic Hardware}


\subsection{Optimality}

Include derivation that spiking LIF + lowpass == rate LIF + lowpass, in the sense of expectation, assuming initial membrane voltages are uniformly distributed.
Also Appendix from NEF text on synaptic versus somatic dynamics.
Validate this numerically looking at intercept-crossings and voltage distributions for higher-frequencies, and scaling properties in terms of $n$ and $\tau$ and firing rates.

Mention the spike-time decoder optimization (Temporal Solver) for more detailed neurons.

Low-rank factorization is optimal in space and time assuming linear weights (there is no advantage to factoring it any further by linearity).


\subsection{Robustness}

Robustness to noise.

Also Conjecture: Den\`eve doesn't work in chaotic settings. NEF does (voltage vector is chaotic). NEF and FORCE are doing the same thing -- the latter just adds extra chaos in because Reservoir Computing.



Expose the general ideas about mapping NEF networks onto neuromorphic hardware, and what makes it better than other frameworks.


\subsection{Extensibility}

Section~\ref{chapt:nef-extensions}.
