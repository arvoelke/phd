\chapter{Introduction}

Computation cannot be physically realized without time.
Whether we consider the discrete switching of voltage states across transistors in a
digital computer,
or the continuous flow of charges across ion channels within the brain -- 
all physical systems, that we currently know of, make progress on their computations
by changing over time.
Digital computers time-multiplex their computations across multiple cores with energetically-expensive access to distant memory, while our brains consume only twenty watts of power by harnessing the dynamics of a biophysical substrate with colocated memory.
An overarching focus of this thesis is a computational paradigm
in between these two extremes: neuromorphic computing.
Here, researchers draw inspiration from how the
brain dynamically performs computations,
to engineer low-power digital and analog circuits that emulate its fundamental principles.

The emerging field of neuromorphic computing has already
witnessed many successes backed by large companies, small start-ups, and new research
programs being established at various institutions all around the world.
Applied Brain Research (ABR), Femtosense, Intel, IBM, and many others~\citep{marketreport2018}, are establishing themselves as key players in the field, while a collaborative landscape is being fostered between a diverse set of academic communities attempting to use such hardware including the Telluride Neuromorphic Cognition Engineering Workshop~\citep{cohen2001report}, Capo Caccia Cognitive Neuromorphic Engineering Workshop, Nengo Summer School (ABR's ``Brain Camp''), and Intel's Neuromorphic Research Community (INRC).
Despite this growing excitement, there exist many challenges when it comes to using
neuromorphic hardware to perform some desired task.
Generally speaking, neuromorphic programming forces us to rethink our traditional,
\citet{von1958}, understanding of computation, and redesign our algorithms to leverage distributed networks of noisy, heterogeneous units, that locally process their inputs and communicate via spikes -- also known as Spiking Neural Networks~(SNNs).
Biological nervous systems already do this quite successfully, and so we seek to borrow existing
principles and models from neuroscience, exploit their properties using tools from mathematics and engineering,
and use software methods to synthesize the resulting networks ``in silico''.
To what end?
By virtue of emulating core principles of brain function, this realizes algorithms that consume far less power, and scale more readily to massively-parallelized computing architectures running in biological real-time.
But such algorithms must also be practically useful.
To flip this on its head: we require frameworks to describe what the hardware is doing at the lowest level,
and tools to systematically harness its capabilities to perform useful computations at the highest level.
Analagous to how a single compiler can translate the same C++ code
to dozens of physically distinct digital architectures, we strive to systematically describe
how the same dynamical computations can be mapped onto distinct spiking neuromorphic architectures.

The primary goal of this thesis is to unpack the above statements through the exposition of fundamental problems and their theoretical solutions, validated by practical results that include a set of novel spiking algorithms deployed on state-of-the-art neuromorphic chips: Braindrop~\citep{braindrop2019} and Loihi~\citep{davies2018loihi}.
This aims to unveil a class of computations where neuromorphic hardware excels, and to further automate and systemetize the process of engineering algorithms for such hardware.
We have already learned a great deal through the mutually interacting processes of bottom-up neuromorphic hardware design and top-down SNN construction.
Theoretical frameworks help guide decisions in hardware design, while hardware constraints demand relevant extensions to theoretical frameworks and software tools -- both processes coexist in a dynamic feedback loop.

Our general approach consists of a software stack, and nonlinear dynamical systems framework,
for describing computations within a language of low-frequency vector spaces.
This approach is known as the Neural Engineering Framework~\citep[NEF;][]{eliasmith2003a}.
The NEF is implemented by the Python package, Nengo~\citep{bekolay2014}, which supports a variety of neuromorphic backends, and is extended by the methods
described in this thesis.
The ultimate goal of this approach is to develop a unified set of tools for constructing
artificially intelligent agents, that scale to the same kinds of tasks that
humans can perform, while running on neuromorphic hardware consuming several orders of magnitude less power than a traditional computer.
Significant progress has been made in this direction over the past two decades -- and although we still
have a long ways to go, this thesis aims to highlight a promising path for the neuromorphic community.

When confronted with ambitious goals such as replicating human intelligence, computer scientists will most often point towards
the popularity of deep learning~\citep{lecun2015deep} and its recent successes such as AlphaGo~\citep{gibney2016google}.
First and foremost, we seek to embrace the methods of deep learning, by incorporating its architectures and training methodoligies into our toolbox.
This is in fact the mandate of Nengo DL~\citep{rasmussen2018nengodl}, a Python package that combines Nengo and Tensorflow into a hybridized framework that automates backpropagation across both spiking, non-spiking, and NEF-style recurrent neural networks -- broadly referred to as the class of Artificial Neural Networks~(ANNs).
However, we do not believe that deeper networks, better training heuristics, nor larger datasets, will be enough to achieve our aforementioned goal.
Concerns for conventional deep learning approaches include: the amount of power required to scale traditional ANNs to the size of the human brain, the difficulties tuning hyperparameters at scale, and the existence of adversarial attacks on networks trained via backpropagation.
But even more fundamentally, the majority of ANNs in use today, including AlphaGo, are applied to \emph{static} inputs to produce \emph{static} outputs.
For a board game such as Go, this is forgiveable, as the entire state of the game is encapsulated by the current state of the board itself, and so there is technically no need to consider any history in order to optimize across future states.\footnote{
Practically speaking, there could still be a benefit, for a sub-optimal player, to knowing the history of the game in order to better predict the opponent's strategy.}
Nevertheless, this points towards a thematic concern: time is not being treated as the dimension of computation in many of the most successful methods, which may limit their applicability to many real-world problems.
For instance, convolutional ANNs are most often deployed in visual processing tasks by classifying objects in single frames.  %, without internal knowledge of how these objects might move or behave.
When processing video, frames are typically sampled at some arbitrary interval and then processed independently from one another.
We will discuss some exceptions later that we embrace.
But at large, most ANNs do not begin with time as a physical dimension intrinsic to the dynamics of the system's input, output, or processing units;
time is neither a variable in the neuron models, synapse models, nor any of the computational elements.
% Consequently, training these networks becomes an art-form than a science, 
% This pattern is beginning to reverse, with examples such as deep LSTMs and qRNNs for machine translation and speech synthesis.
% However, we argue that we can do even better, by arming ourselves with tools for better understanding the role of time, and being able to engineer specific dynamic computations into RNNs.

On the other hand, the human brain is a physical system that must continuously process information over time.
This is crucial for a cognitive agent embodied in a dynamic environment wherein all interactions depend on internally and externally evolving states.
The brain naturally learns from and interacts with its environment over many different time-scales. 
For instance, our brains are not programmed to recognize static objects at discrete moments in time.
Rather, we have evolved to interact with objects in continuously changing environments, from which the perception of object recognition emerges.
From a very young age, children can intuitively infer the physics of moving objects, and learn how to interact with them to achieve desired outcomes.
This continues well through adulthood, up to the level of coordinating our thoughts and behaviours, shaped by our cumulative experience with the world.
% Everything that we connect with helps to shape our brains to conceptualize these changes.
This occurs so naturally that we often take it for granted, but at a very basic basic level, our brains are necessarily constructing explicit and implicit dynamical models of reality, and applying these models
to perform behaviourally relevant computations in real-time.

Consider a real-world example such as driving a car. We must constantly integrate new information with our existing understanding of where we are, where we are headed, and what might happen along the way.
This requires not only an intuitive understanding of the physics of the car, but models of how other drivers behave on the road, with respect to changing traffic lights and road conditions, that play out in parallel while we flexibly plan and coordinate our actions.
Likewise, consider any actively engaging task or hobby that you enjoy.
I speculate this activity requires some amount of mentally or physically coordinated interaction with the world, on a similar level of dynamic complexity as in the driving example, such as: playing a video game, participating in a sport, engaging with some form of media, playing music, drawing, dancing, and so forth.
Besides such activities being meaningful, fun, and beautiful, they all share the common motif of recruiting a variety of systems in a dynamically coordinated fashion.
% Abstractly, we would like to conceptualize this as a compression hierarchy and decompression hierarchy whose two outside ends are coupled in time. Inside is ones own personal representation of the activity (see Fig. ???). At the core, the system strives to create the stable representation of an `illusion' that all of this takes place at once.
% If one is having fun, then this internal representation will try its best to encompass both ends.

To help appreciate how extraordinary such tasks are for the human brain, the mechanisms involved do not have a perfect memory of past inputs.
Each neuron locally processes some signal by encoding its input current into spike trains.
These action potentials evoke post-synaptic currents~(PSCs) that decay exponentially on the order of milliseconds.
The time-scales over which these mechanisms interface with the world, and interact with each other, determines, and constrains, literally everything that we can do.
Memories emerge from these dynamics, as changes in neural activity and synaptic weights reflect the history of internal representations along a multitude of time-scales.
Most impressively of all, the collection of approximately one-hundred billion neurons, connected by over one-hundred trillion synapses, process these signals sparsely across space and time using around twenty watts~\citep{koch2014} -- about the same energy as a typical household CFL bulb.

This ``signal processing view'' of computation is fundamentally different from conventional views of computation targeted for conventional computers.
In the former view, time is intrinsic to the state of all physical ``devices'' performing the computation~(i.e.,~neurons and synapses), which are themselves necessarily tied to the timing and dynamics of the overall function.
However, in the latter view, the time that a computation takes is an implementation detail that is mostly decoupled from the actual function.
As such, there is currently an important distinction between the computational approaches taken by the neuromorphic community and those who program traditional digital computers.
This is not simply a matter of biological detail, but more crucially a matter of thinking of the system that is actually solving these tasks as a collection of signal processors whose dynamics are coupled to the task at hand by the physics of time.
If our aim is to build artificial systems that are as useful, robust, capable, and scalable as the human brain, then we should strive to understand computation in such terms.

The approach we take considers time as a fundamental starting point, and thus situates dynamics front-and-center; we bridge the time-scales of the entire system, down from the level of spikes and post-synaptic currents, up to the level of human reaction times and behaviours. 
We validate this model of computation by deploying these SNNs on low-power neuromorphic hardware emulating core biological principles.
% There is no arithmetic logic unit, random-access memory, or instruction set.
This involves developing an extensible theoretical framework that links together the mechanisms of an SNN, its parameters, and some desired class of computations, while embracing all of the tools at our disposal.

%Another significant motivating factor for this work is the recent surge in neuromorphic computing architectures such as SpiNNaker~\citep{mundy2015efficient}, Neurogrid~\citep{choudhary2012silicon}, Brainstorm~\citep{brainstorm}, and IBM's TrueNorth~\citep{merolla2014million}.
%These platforms are massively-parallel low-power analog and/or digital systems that are designed specifically to simulate large-scale SNNs rivalling the complexity of the human brain.
%Despite the growing excitement surrounding these platforms, the community is struggling to fully unlock the computational power of this hardware.
%We believe this is primarily the result of peoples' inability to reconcile their discrete {\it von Neumann}-like understanding of conventional algorithms with the continuous brain-like processing of neuromorphic hardware.
%To take full advantage of such hardware, our `compilers' must account for the effects of simulated neuron models and synaptic models at the system level, and moreover exploit these effects in useful ways whenever possible.
%This has forced us to come full circle; once again we must understand computation in terms of the brain's processing units.

%The remainder of this thesis will be organized as follows.
%We will first survey existing methods of building RNNs (both spiking and non-spiking) and recent neuromorphic hardware architectures.
%We will then review the main ideas and tools that we use from control theory, linear systems theory, nonlinear dynamics, and systems design.
%Therefore, this thesis will theoretically explore ideas and to solve problems that arise from taking the above view of how the brain processes information. This will be supplemented by numerical simulations which validate theories, specific examples which demonstrate applicability, and connections to experimental studies which support or constrain our models when appropriate.
%Many of these problems are driven by current unpublished challenges within the neuromorphic community.
%A secondary goal is to demonstrate that these methods are practically useful by surpassing state-of-the-art results in machine learning, although we will not consider it a failure if this is not achieved within the remaining time span of the thesis, since there is still much ground to cover in theoretical foundations.
%Overall we aim to reveal new insights and make new methods widely available, that we hope will prove useful to engineers, theorists, and practitioners alike.

