\chapter{Conclusions}
\label{chapt:conclusions}

We have discussed a number of theoretical and practical results involving the synthesis of dynamical systems, spiking neural networks, and neuromorphic hardware.
We now summarize our main contributions in order.

First, section~\ref{sec:sub-principles} observed a number of computational ``sub-principles'' that follow from the adoption of the NEF's three principles.
Arbitrary network topologies reduce to a single recurrently connected layer with sparse encoders and decoders with block structure isomorphic to the original graph structure.
Heterogeneous dynamical primitives form the basis for neural computation, and may be expressed within a unified language to facilitate mathematical analyses that leverage the interchangeability and composibility of such primitives.
Chaotic strange attractors emerge from even the simplest spiking implementations of such dynamical systems.
The NEF represents state-vectors by linearly projecting them onto the postsynaptic currents of neurons,  independent of any considerations of what it means to use a ``rate code'' or a ``spike-time code''.
Likewise, these state-vectors represent frequency content that grows linearly with the relative amount of energy required to drive the synapses of each postsynaptic neuron, for a fixed level of precision.

Second, section~\ref{sec:nef-suitability} addressed the suitability of NEF as a framework for compiling SNNs onto neuromorphic hardware.
Correctness is guaranteed by Theorem~\ref{thm:correctness}, which provides a novel proof of the scaling of the NEF's precision, conditioned upon a specific criteria characterizing the distribution of neural states.
Scalability is guaranteed by the previous theorem, in conjunction with a number of prior observations made about time, space, and energy requirements (Table~\ref{tab:scalability}).
Completeness is provisioned by the Turing-completeness of dynamical systems, which justifies the use of spiking neural networks---trained using the NEF to approximate the same dynamics---as powerful models of computation.
Robustness is ensured by a volume of prior work, together with our novel observation that NEF is robust to neural attractor dynamics.
Extensibility is demonstrated by a large number of Nengo backends demonstrating NEF networks functioning on a variety of seemingly disparate architectures, in addition to the extensions summarizes below.

Third, section~\ref{sec:dynamics-language} provided a number of novel perspectives on computations that are not normally viewed from a dynamical systems-based perspective.



We have discussed two main theoretical results.
The first provides a method for accurately implementing continuous-time delays in recurrent spiking neural networks.
This begins with a model description of the delay system, and ends with a finite-dimensional representation of the input's history that is mapped onto the dynamics of the synapse.
The second provides a method for harnessing a broad class of synapse models in spiking neural networks, while improving the accuracy of such networks compared to standard NEF implementations.
These extensions are validated in the context of the delay network.

Our extensions to the NEF significantly enhance the framework in two ways.
First, it allows those deploying the NEF on neuromorphics to improve the accuracy of their systems given the higher-order dynamics of mixed-analog-digital synapses~\citep{voelker2017iscas, voelker2017neuromorphic}.
Second, it advances our understanding of the effects of additional biological constraints, including finite rise-times and pure time-delays due to action potential propagation.
Not only can these more sophisticated synapse models be accounted for, but they may be harnessed to directly improve the network-level performance of certain systems.

We exploited this extension to show that it can improve the accuracy of discrete-time simulations of continuous neural dynamics.
We also demonstrated that it can provide accurate implementations of delay networks with a variety of synapse models, allowing systematic exploration of the relationship between synapse- and network-level dynamics.
Finally we suggested that these methods provide new insights into the observed temporal properties of individual cell activity.
Specifically we showed that time cell responses during a delay task are well-approximated by a delay network constructed using these methods.
This same delay network nonlinearly encodes the history of an input stimulus across the delay interval (i.e.,~ a rolling window) by compressing it into a $q$-dimensional state, with length scaling as $\bigoh{ \frac{q}{f} }$, where $f$ is the input frequency.

While we have focused our attention on delay networks in particular, our framework applies to any linear time-invariant system.
As well, though we have not shown it here, as with the original NEF formulation these methods also apply to nonlinear systems.
As a result, these methods characterize a very broad class of combinations of synapse- and network-level spiking dynamical neural networks.

\section{Future Directions}

Many important questions still remain concerning the interactions between Principles~1,~2, and~3.
While the error in our transformations scale as $\bigoh{ \frac{1}{\sqrt{n}} }$ due to independent spiking, it has been shown that near-instantaneous feedback may be used to collaboratively distribute these spikes and scale the error as $\bigoh{ \frac{1}{n} }$~\citep{boerlin2013predictive, thalmeier2016learning}.
This reduction in error has potentially dramatic consequences for the efficiency and scalability of neuromorphics by reducing total spike traffic~\citep{boahen2017neuromorph}.
However, it is currently unclear whether this approach can be applied to a more biologically plausible setting (e.g.,~using neurons with refractory periods) while retaining this linear scaling property.
Similarly, we wish to characterize the network-level effects of spike-rate adaptation, especially at higher input frequencies, in order to understand the computations that are most accurately supported by more detailed neuron models.
This will likely involve extending our work to account for nonlinear dynamical primitives and subsequently harness their effects (e.g.,~bifurcations) to improve certain classes of computations.

Delay network to store and replay episode memories (temporal semantic pointer)

Venn diagram showing intersection between biology, hardware, and what is useful?

Make useful: dendritic computation, adaptation.

Encode higher-frequency information.

Encode general temporal features (e.g., extending delay network to other basis functions).

More principled energy-minimizing network construction methods

\subsection{Energy Minimization}

``Differential encoding'' (adaptation)?

Spike-thinning (accumulator on BrainDrop)

Improving scaling by spreading out the spikes (inspired by Den\`eve)

Interneurons on Loihi

Other ideas: attentional routing (inhibition to the source). Dynamically adjusting firing rates from feedback error. Better automated selection of tuning curves (e.g., finding thresholds latent within the decomposition of functions)

\TODO{spike delays for minimizing PSC variability, like in Deneve's methods}
