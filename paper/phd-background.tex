\chapter{Background}
\label{chapt:background}

\section{Recurrent Neural Networks}

\subsection{Traditional Approaches}

Backpropagation through time (unrolling)

Neural variants: Hunsburger, nengo\_dl, Maass, Hugh \& Sjenowski

\subsection{Structured Approaches}

LSTM, GRU, qRNN

\subsection{Unsupervised Approaches}

SORN, Numenta

\subsection{Reservoir Computing}

LSM, ESN

\subsection{Supervised State-Space Learning}

Duggins and activity-space extensions

FORCE, full-FORCE

Wilten Nicola

Aditya \& Gerstner, PES

\subsection{Balanced Networks}

Den\`eve, Memmesheimer, Slotine

\subsection{Others}

Friedemann Zen
Recurrent Information Optimization with Local, Metaplastic Synaptic Dynamics (ShiNung Ching)
System Level Approach (SLA) https://arxiv.org/abs/1701.05880


\section{Neural Engineering Framework}

\subsection{Principle 1}

Anectodally (e.g., personal conversation with Sophie Den\`eve, experience with Nengo, and averages in Spaun), we say 50-100 neurons per dimension achieves tolerable levels of noise.
For a fixed amount of noise in a represention implemented by an ensemble of $n$ neurons, the current best-known lower-bound predicts the dimensionality $d$ scales as $\Omega \left( n^{\frac{2}{3}} \right)$ for $n$ neurons~citep[][p.~60]{jgosmann2018}.
Although we don't know the upper-bound, we conjecture that it is $\bigoh{n}$.
That is, the dimensionality should not be able to scale faster than the neuron count.
If it could, then each neuron would represent $\omega(1)$ dimensions, which seems physically implausible given access to only $\bigoh (1)$ state variables.
This limitation could be broken by dendritic computation, for instance if each neuron had access to $\bigoh (n)$ variables distributed along the dendritic tree.

\subsection{Principle 2}

\subsection{Principle 3}


\section{Neuromorphic Computing}

Common goals.
Colocated memory and computation
Heterogeneity and sparsity in time
Heterogeneity and sparsity in space
Minimize spiking / synaptic events
Minimize power
Class of computations / descriptions
https://ieeexplore.ieee.org/document/7551407/ Christian Mayr's digital SRAM scaling 

\subsection{SpiNNaker}

1 and 2
Stromatias2013
Furber2014

\subsection{Braindrop}

and Neurogrid

\subsection{Loihi}

\subsection{Others}

Spikey, BrainScaleS 1 and 2, Dynapse 1 and 2, TrueNorth, DeepSouth, COLAMN, ODIN, ROLLS, Giacomo and Eliasmith, Tripp, Wang and Tapson, STPU, Neuromemristive random projection networks (Dhireesha Kudithipudi)


\section{Dynamical Systems}

\subsection{Linear Time-Invariant Systems}

State-space representations, transfer functions, filters, convolution, properties, Pad\'e approximants and coordinate transformations

\subsection{Nonlinear Systems}

Linearization, Jacobians, signatures of chaos

\subsubsection{Coordinate Transformation}

\begin{theorem}
Let $\V{f}(t)$ and $\V{g}(t)$ be infinitely time-differentiable signals, that are related by:
\begin{equation} \label{eq:f}
\V{f} = \sum_{i=0}^\infty c_i \V{g}^{(i)} \text{,}
\end{equation}
for some coordinates $\coords{c}{i}$. Then this dynamical system is equivalent to:
\begin{equation} \label{eq:g}
\V{g} = \sum_{i=0}^\infty b_i \V{f}^{(i)} \text{,}
\end{equation}
where the coordinates $\coords{b}{i}$ are defined by the recursive transformation:
\begin{equation} \label{eq:b}
b_i = c_0^{-1} \begin{cases}
    1 & i = 0 \\
    %- (b \ast c)\left[ i \right] & i \ge 1 ,
    - \sum_{j=0}^{i-1} b_j c_{i - j} & i \ge 1 \text{.}
  \end{cases}
\end{equation}
%and $(b \ast c)\left[ i \right] := \sum_{j=0}^{i-1} b_j c_{i - j}$ is a discrete convolution. % that depends on $b_j$ for all $0 \le j \le i - 1$.
\end{theorem}

\begin{corollary}
Let: $$H_c(s) = \frac{1}{\sum_{i=0}^\infty c_i s^i}, \quad H_b(s) = \frac{1}{\sum_{i=0}^\infty b_i s^i}, $$ where $b$ is defined by (\ref{eq:b}). Then (\ref{eq:f}) is equivalent to $F(s)H_c(s) = G(s)$ and similarly (\ref{eq:g}) is equivalent to $G(s)H_b(s) = F(s)$, and moreover:
\begin{equation} \label{eq:inv}
H_c(s) H_b(s) = 1 \text{,}
\end{equation}
hence $H_c(s)$ and $H_b(s)$ are eachother's reciprocals. Furthermore, the coordinate transformation (\ref{eq:b}) is its own inverse.
\end{corollary}

\begin{proof}
Noting that $\V{g}^{(0)} = \V{g}$, rearrange (\ref{eq:f}) as:
\begin{equation} \label{eq:gf}
\V{g} = c_0^{-1} \V{f} + \sum_{i=1}^\infty (-c_0^{-1} c_i) \V{g}^{(i)} \text{.}
\end{equation}
Differentiate each side an infinite number of times, to obtain the following set of equations that hold for all $j \in \mathbb{N}$:
\begin{equation} \label{eq:dg}
\V{g}^{(j)} = c_0^{-1} \V{f}^{(j)} + \sum_{i=1}^\infty (-c_0^{-1} c_i) \V{g}^{(i+j)} \text{.}
\end{equation}
Now recursively substitute (\ref{eq:dg}) into (\ref{eq:gf}) for all occurrences of $\V{g}^{(i)}$, $i \ge 1$ until we are only left with $\V{f}^{(j)}$ terms for $j \le i$, and take the limit as $i \rightarrow \infty$. This is equivalent to treating (\ref{eq:dg}) as a recursive function in $j$ and then evaluating $\V{g}^{(0)}$.
Although this is infinitely generative from a bottom-up perspective, we are `allowed' to do this because each substitution of (\ref{eq:dg}) increases the order of $\V{g}$ by 1 (hence it terminates top-down). Another way to view this is we pick some finite $k$ in order to gather all occurrences of $\V{f}^{(k)}$ in the infinite expansion. After repeating this for all $k \rightarrow \infty$, we get something of the form:
\begin{equation*}
\V{g} = \sum_{k=0}^\infty \tilde{b}_k \V{f}^{(k)} \text{.}
\end{equation*}
Now, all that remains is to show that $\tilde{b}_k = b_k$ from (\ref{eq:b}) in order to match (\ref{eq:g}). We do this inductively in a way that parallels the recursive structure of the substitution procedure.

For the base case ($k = 0$) it should be clear that $\tilde{b}_0 = c_0^{-1} = b_0$ since this is the only way to construct $\V{f}^{(0)}$.
For the inductive case ($k \ge 1$), the only way to construct $\V{f}^{(k)}$ is through the substitution of $\V{g}^{(k)}$ in (\ref{eq:dg}).
Furthermore, this occurs for all $0 \le j \le k - 1$. In particular, for every such $j$, we have the coefficient $(-c_0^{-1} c_{i})$ multiplied by $\V{g}^{(i + j)}$ to yield $c_0^{-1} \V{f}^{(k)}$ where $k = i + j$.
Finally, this occurs $\tilde{b}_{j} c_0$ times since it is mirrored by each occurrence of $\V{f}^{(j)}$.
Putting this all together, we get $\tilde{b}_k = \sum_{j=0}^{k-1} -c_0^{-1} c_{k - j} \tilde{b}_j = c_0^{-1} \sum_{j=0}^{k-1} - b_j c_{k - j} = b_k$ (by the inductive hypothesis).
\end{proof}

\TODO{Move Example later on when citing ISCAS paper}

This theorem is a more general version of the analysis we do in ISCAS~2017. To be concrete, we have:
$$
\V{f} = (\epsilon \gamma)^{-1} \left( \V{x} + (\tau_1 + \tau_2) \dot{\V{x}} + \tau_1 \tau_2 \ddot{\V{x}} \right) \text{,}
$$
$$
c_i = \frac{(-\epsilon)^i}{(i + 1)!} \text{,}
$$
and $\V{g}$ is the signal being used to drive the pulse-extender, such that $\V{f} = \sum_{i=0}^\infty c_i \V{g}^{(i)}$ (see my other notes on improving Kwabena's box filter analysis).

In my refined analysis, we only keep the terms $i \le 2$ (or $i \le 1$ in the paper), but we can still use the above coordinate transformation as a sanity check. In particular, we have:
$$
c_0 = 1, \quad c_1 = -\frac{\epsilon}{2}, \quad c_2 = \frac{\epsilon^2}{6}, \quad c_3 = -\frac{\epsilon^3}{24}, \quad c_i \approx 0 \quad \forall i \ge 4 \text{.}
$$
Then using the transformation (\ref{eq:b}), we get:
$$
b_0 = 1, \quad b_1 = \frac{\epsilon}{2}, \quad b_2 = \frac{\epsilon^2}{4} - \frac{\epsilon^2}{6} = \frac{\epsilon^2}{12}, \quad b_3 = \frac{\epsilon^3}{24} - \frac{\epsilon^3}{12} + \frac{\epsilon^3}{24} = 0 \text{,}
$$
and so:
\begin{align*}
\V{g} &= \sum_{i=0}^\infty b_i \V{f}^{(i)} \\
        &= \V{f}^{(0)} + \frac{\epsilon}{2} \V{f}^{(1)} + \frac{\epsilon^2}{12} \V{f}^{(2)} + \ldots \\
        &= (\epsilon \gamma)^{-1} \left( \V{x} + (\tau_1 + \tau_2 + \epsilon / 2) \dot{\V{x}} + (\tau_1 \tau_2 + (\epsilon / 2 ) (\tau_1 + \tau_2) + \epsilon^2 / 12) \ddot{\V{x}} \right) + \ldots
\end{align*}

The coordinate transformation (\ref{eq:b}) is equivalent to finding the Pad\'e approximants when the order of the numerator is $p = 0$.
To be precise,
$$
\sum_{i=0}^{q} c_i x^i \approx \frac{1}{\sum_{i=0}^q b_i x^i}
$$
is best-approximated by taking the Pad\'e approximants:
$$
\coords{b}{i=0 \ldots q} = \left[ 0 / q \right] \sum_{i=0}^{q} c_i x^i \text{.}
$$
This can be seen through (\ref{eq:inv}) and verified by comparing the algorithm for the extended euclidean algorithm for the polynomial GCD, in this special case, to the algorithm (\ref{eq:b}). That is, they both implement long division.

Despite the fact that we essentially re-derived a special case of the Pad\'e approximants, this still leads to some non-trivial insights:

The coordinate transformation (\ref{eq:b})---that is, the algorithm for finding the Pad\'e approximants when $p = 0$---is its own inverse. That is, we may convert back and forth using the same transformation without any loss of information.

We may also interpret (\ref{eq:b}) as a discrete dynamical system by realizing that $b$ is the discrete convolution of itself with $c$. With some work, we can show that this is the same as stating that $\ztrans{b}\ztrans{c} = 1$, or equivalently in the time-domain, $\coords{b}{i=0\ldots q}$ is the impulse-response of the following discrete transfer function:
$$
H^q_{c \rightarrow b}(z) = \frac{z^{q}}{\sum_{i=0}^q c_i z^{q - i}} \text{,}
$$
and likewise, $H^q_{b \rightarrow c}(z)$ has the impulse-response $\coords{c}{i=0\ldots q}$.
Furthermore, $$\lim_{q \rightarrow \infty} H^q_{c \rightarrow b}(z) H^q_{b \rightarrow c}(z) = 1,$$ analogous to (\ref{eq:inv}).
