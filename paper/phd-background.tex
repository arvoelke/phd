\chapter{Background}
\label{chapt:background}

\section{Recurrent Neural Networks}

\subsection{Traditional Approaches}

Backpropagation through time (unrolling)

Neural variants: Hunsburger, nengo\_dl, Maass, Hugh \& Sjenowski

\subsection{Structured Approaches}

LSTM, GRU, qRNN

\subsection{Unsupervised Approaches}

SORN, Numenta

\subsection{Reservoir Computing}

LSM, ESN

\subsection{Supervised State-Space Learning}

Duggins and activity-space extensions

FORCE, full-FORCE

Wilten Nicola

Aditya \& Gerstner, PES

\subsection{Balanced Networks}

Den\`eve, Memmesheimer, Slotine

\subsection{Others}

Friedemann Zen
Recurrent Information Optimization with Local, Metaplastic Synaptic Dynamics (ShiNung Ching)
System Level Approach (SLA) https://arxiv.org/abs/1701.05880


\section{Neural Engineering Framework}

\subsection{Principle 1}

Anectodally (e.g., personal conversation with Sophie Den\`eve, experience with Nengo, and averages in Spaun), we say 50-100 neurons per dimension achieves tolerable levels of noise.
For a fixed amount of noise in a represention implemented by an ensemble of $n$ neurons, the current best-known lower-bound predicts the dimensionality $d$ scales as $\Omega \left( n^{\frac{2}{3}} \right)$ for $n$ neurons~citep[][p.~60]{jgosmann2018}.
Although we don't know the upper-bound, we conjecture that it is $\bigoh \left( n \right)$.
That is, the dimensionality should not be able to scale faster than the neuron count.
If it could, then each neuron would represent $\omega(1)$ dimensions, which seems physically implausible given access to only $\bigoh (1)$ state variables.
This limitation could be broken by dendritic computation, for instance if each neuron had access to $\bigoh (n)$ variables distributed along the dendritic tree.

\subsection{Principle 2}

\subsection{Principle 3}


\section{Neuromorphic Computing}

Common goals.
Colocated memory and computation
Heterogeneity and sparsity in time
Heterogeneity and sparsity in space
Minimize spiking / synaptic events
Minimize power
Class of computations / descriptions
https://ieeexplore.ieee.org/document/7551407/ Christian Mayr's digital SRAM scaling 

\subsection{SpiNNaker}

1 and 2

\subsection{Braindrop}

and Neurogrid

\subsection{Loihi}

\subsection{Others}

Spikey, BrainScaleS 1 and 2, Dynapse 1 and 2, TrueNorth, DeepSouth, COLAMN, ODIN, ROLLS, Giacomo and Eliasmith, Tripp, Wang and Tapson, STPU, Neuromemristive random projection networks (Dhireesha Kudithipudi)


\section{Dynamical Systems}

\subsection{Linear Time-Invariant Systems}

State-space representations, transfer functions, filters, convolution, properties, Pad\'e approximants and coordinate transformations

\subsection{Nonlinear Systems}

Linearization, Jacobians, signatures of chaos

